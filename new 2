跟吴振峰讨论了一下.nlp语句去重的问题:

数据:100w个句子

算法:
1.分词,然后去掉停用词,停用词除了一般汉子停用词外,还需要算上,数据集中出现频率超低的词汇,和出现频率超高的词汇.
一种理由是出现频率超高表示没有区分度的意义,出现频率超低1-2词,的很有可能是错别字.


2.根据词频建立词库3万个词的字典,key就是1到3万,value是词
3.根据字典,对每一句话进行编码
4.进行bitmap编码压缩存储空间.bitmap有3万个位,每一个位置上有1表示,这个index对应字典中的词在句子中出现了
5.利用kmean进行处理,聚类100类 (距离函数是,两个bitmap做异或后结果的按位求和,这里规定结果大于4就表示两个句子不一样)
6.每一类进行N^2复杂度去重,得到的句子总体记做M1
7.对于每一个聚类,取距离中心最远的100的点,记做M2
8.M2总共有一万个点,M2内部做一个N^2复杂度去重,得到M3
9.M1+M3就是最后需要的句子.
10.总结:这里面用到了贪心,数据如果分布极其怪异会仍然得到有重复的结果
11.总复杂度是100亿次,密级是N^1.66








第二种:
算法:
1.分词,然后去掉停用词
2.根据词频建立词库3万个词的字典,key就是1到3万,value是词
3.根据字典,对每一句话进行编码
4.进行bitmap编码压缩存储空间.bitmap有3万个位,每一个位置上有1表示,这个index对应字典中的词在句子中出现了
5.利用dbscan进行处理
	时间复杂度：

	（1）DBSCAN的基本时间复杂度是 O(N*找出Eps领域中的点所需要的时间), N是点的个数。最坏情况下时间复杂度是O(N2)

	（2）在低维空间数据中,有一些数据结构如KD树，使得可以有效的检索特定点给定距离内的所有点，时间复杂度可以降低到O(NlogN)

6.考虑这个算法的复杂度,这个算法可以严格去重,但是KDtree对于大数据,时候树的建立非常慢.







